{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem.rdmolops import FindPotentialStereo\n",
    "# Alter here only\n",
    "\n",
    "# Cria a variável que contem a lista de arqquivos que voce quer olhar, se for só um deixa como string\n",
    "path_to_files = \"./duplicate_analysis\" #\"D:\\\\Projetos academicos\\\\Labmol\\\\Code\\\\Data\\\\My_data\\\\Clinical Data\\\\Bundled_Data\\\\results\"\n",
    "# To get from a directory                          \n",
    "# troca os.listdir por um método semelhante que percorra o drive\n",
    "files = [f\"{path_to_files}/{file}\" for file in os.listdir(path_to_files)]\n",
    "print(files)\n",
    "name = [f\"{file.strip('.xlsx')}\" for file in os.listdir(path_to_files)]\n",
    "name = [f\"{n.strip('.csv')}\" for n in name]\n",
    "your_smiles = \"Chemical Structure\"\n",
    "your_values = \"pLD50\"\n",
    "your_study_type = \"pLD50\"\n",
    "result_name=\"Sparse_Birds\"\n",
    "your_endpoint_names = [\"\"] # if you want actual names and not indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def pandas_convert_excel_to_csv(files : str or list):\n",
    "    if type(files) is list:\n",
    "        for file in files:\n",
    "            if file.find(\".xlsx\")!=-1:\n",
    "                sheets = pd.ExcelFile(file).sheet_names\n",
    "                for sheet in sheets:\n",
    "                    df = pd.read_excel(file,sheet_name=sheet)\n",
    "                    df.to_csv(f\"./temp_dataset/{file}_{sheet}.csv\")\n",
    "    else:\n",
    "        if files.find(\".xlsx\")!=-1:\n",
    "            sheets = pd.ExcelFile(files).sheet_names\n",
    "            for sheet in sheets:\n",
    "                df = pd.read_excel(files,sheet_name=sheet)\n",
    "                df.to_csv(f\"./temp_dataset/{files}_{sheet}.csv\")\n",
    "try:\n",
    "    os.mkdir(\"./temp_dataset\")\n",
    "    pandas_convert_excel_to_csv(files)\n",
    "    s_file = [f\"{path_to_files}\\\\{file}\" for file in os.listdir(\"./temp_dataset\")]\n",
    "except FileExistsError:\n",
    "    pandas_convert_excel_to_csv(files)\n",
    "    s_file = [f\"{path_to_files}\\\\{file}\" for file in os.listdir(\"./temp_dataset\")]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def organize(all_df_list):\n",
    "    #smiles_list=[]\n",
    "    #for df_name in all_df_list:\n",
    "        \n",
    "    #df_smiles=[]\n",
    "    for i,df_name in enumerate(all_df_list):\n",
    "        if df_name.find(\".sheet\")!=-1:\n",
    "            file = df_name.split(\".sheet\")\n",
    "            print(file)\n",
    "            df = pd.read_excel(file[0],sheet_name=file[1])\n",
    "        else:\n",
    "            df = pd.read_csv(df_name)\n",
    "\n",
    "\n",
    "        smiles = df.loc[:,your_smiles]\n",
    "        d_f = df.dropna(axis=0, how='all')\n",
    "        #df = pd.DataFrame([inchik,canon_smiles,df.loc[:,new_values]],columns=[\"Inchi\",\"SMILES\",new_values])\n",
    "        canon_smiles = []\n",
    "        inchik = []\n",
    "        #smiles_s = df[smiles_col]\n",
    "        for ds in smiles:\n",
    "            try:\n",
    "                cs = Chem.CanonSmiles(ds)\n",
    "                mol = Chem.MolFromSmiles(cs)\n",
    "                inchik.append(Chem.rdinchi.MolToInchiKey(mol))\n",
    "                canon_smiles.append(cs)\n",
    "            except:\n",
    "                d_f.drop(index=i,inplace=True)\n",
    "                d_f.reset_index(drop=True, inplace=True)\n",
    "        #print(len(inchik))\n",
    "        new_values = f\"{your_study_type}_{i}\"              \n",
    "        df = pd.DataFrame()\n",
    "        df[\"Inchi\"] = inchik\n",
    "        df[\"SMILES\"] = canon_smiles\n",
    "        df[new_values] = d_f.loc[:,your_values]\n",
    "        #df = df.drop(your_smiles, axis=1)\n",
    "        \n",
    "        #smiles_col = f\"Smiles {i}\"\n",
    "\n",
    "        \n",
    "        #df = df.rename(columns={your_values:new_values})#, your_smiles:smiles_col})\n",
    "        \n",
    "        #mols = []\n",
    "        \n",
    "        \n",
    "        # if i > 1:\n",
    "        #     #df_smiles.clear()\n",
    "        # # df_all_inchi=pd.merge(df_all_inchi,df, how=\"outer\",on=\"SMILES\")\n",
    "        #     df_all_inchi = pd.merge(df_all_inchi, df, how=\"outer\", on=[\"Inchi\",\"SMILES\"])\n",
    "        if i == 0:\n",
    "            #df_smiles[i] = df\n",
    "            df_all_inchi = df\n",
    "        elif i >= 1: \n",
    "            #df_smiles[i] = df\n",
    "            #prev_df = df_smiles[i-1]\n",
    "            # df_all_inchi=pd.merge(df,prev_df, how=\"outer\", on=\"SMILES\")\n",
    "            try:\n",
    "                df_all_inchi = df_all_inchi.merge(df, how = \"outer\", on = [\"Inchi\",\"SMILES\"],validate=\"one_to_one\")\n",
    "            except:\n",
    "                merged_inchi = df_all_inchi[\"Inchi\"]\n",
    "                for inchi in df[\"Inchi\"]:\n",
    "                    if inchi in merged_inchi:\n",
    "                        df.drop(index=inchi,inplace=True)\n",
    "\n",
    "                df_all_inchi = df_all_inchi.merge(df, how = \"outer\", on = [\"Inchi\",\"SMILES\"])\n",
    "            \n",
    "            df_all_inchi.drop_duplicates(\"Inchi\",inplace=True)\n",
    "            print(f\"File {df_name}\")\n",
    "            print(f\"Df size {df.shape[0]}\")\n",
    "            print(f\"Df_inchi Size {df_all_inchi.shape[0]}\")\n",
    "    id_=[f\"'{num}'\" for num in range(df_all_inchi.shape[0])]    \n",
    "    df_all_inchi.insert(0,\"ID\",id_)\n",
    "    df_all = df_all_inchi.drop(\"Inchi\", axis=1)\n",
    "    \n",
    "    return df_all\n",
    "    \n",
    "\n",
    "def read_through_files(files):\n",
    "        dfs = []\n",
    "        names = []\n",
    "        if type(files) is str:\n",
    "            # name=result_name\n",
    "            # name=name.replace(\".xlsx\",\"\")\n",
    "            # name=name.replace(\".csv\",\"\")\n",
    "            if files.find(\".csv\") != -1:\n",
    "                #names.append(name)\n",
    "                dfs.append(f\"{files}\")#pd.read_csv(files,delimiter=\",\"))\n",
    "                #organize(df,name)\n",
    "            else:\n",
    "                sheets = pd.ExcelFile(files).sheet_names\n",
    "                if len(sheets)>1:\n",
    "                    for sheet in sheets:\n",
    "                        if sheet != \".\":\n",
    "                            #names.append(sheet)\n",
    "                            dfs.append(f\"{files}.sheet{sheet}\")#pd.read_excel(files,sheet_name=sheet))\n",
    "                            #organize(df,name)\n",
    "                else:\n",
    "                    for sheet in sheets:\n",
    "                        if sheet != \".\":\n",
    "                            #name.append(files)\n",
    "                            dfs.append(f\"{files}.sheet{sheet}\")#pd.read_excel(files,sheet_name=sheet))\n",
    "        else:\n",
    "            for file in files:\n",
    "                if file.find(\".csv\") != -1:\n",
    "                   # names.append(files)\n",
    "                    dfs.append(f\"{file}\")#pd.read_csv(file,delimiter=\",\"))\n",
    "                else:   \n",
    "                    sheets = pd.ExcelFile(file).sheet_names\n",
    "                    if len(sheets)>1:\n",
    "                        for sheet in sheets:\n",
    "                            if sheet != \".\":\n",
    "                                #names.append(sheet)\n",
    "                                dfs.append(f\"{file}.sheet{sheet}\")#pd.read_excel(file,sheet_name=sheet))\n",
    "                                #organize(df,name)\n",
    "                    else:\n",
    "                        for sheet in sheets:\n",
    "                            if sheet != \".\":\n",
    "                                #name.append(file)\n",
    "                                dfs.append(f\"{file}.sheet{sheet}\")#pd.read_excel(file,sheet_name=sheet))\n",
    "        return organize(dfs)\n",
    "df = read_through_files(files=files)\n",
    "#df.fillna(np.nan,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = df\n",
    "corr = correlation_df.corr()\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(font_scale=1.9, rc={'figure.figsize':(30,25)})\n",
    "sns.heatmap(corr, cmap='RdYlBu_r', vmin=-1, vmax=1, annot=True)\n",
    "plt.title('Correlation Heatmap', fontsize=20)\n",
    "plt.savefig('Correlation.png', bbox_inches='tight',\n",
    "            transparent=True, format='png', dpi=500)\n",
    "\n",
    "# convert back to id numbers\n",
    "correlation_df[\"ID\"] = [int(str_.strip(\"'\")) for str_ in correlation_df[\"ID\"]]\n",
    "correlation_df.to_csv(f\"./results/{result_name}.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fa495dfb2e0dadd8ed0afe6080159ac5bdc526ac154811e8f369ccca6201803"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DLModels')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
